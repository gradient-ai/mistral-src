{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da54d44c-9f04-41cb-b16a-ef557a59c26e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:47:26.201664Z",
     "iopub.status.busy": "2024-02-23T13:47:26.200918Z",
     "iopub.status.idle": "2024-02-23T13:49:28.727787Z",
     "shell.execute_reply": "2024-02-23T13:49:28.726857Z",
     "shell.execute_reply.started": "2024-02-23T13:47:26.201628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.2.1 which is incompatible.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbc02d6-1e59-41a4-a475-b772e69688ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:49:37.433714Z",
     "iopub.status.busy": "2024-02-23T13:49:37.433058Z",
     "iopub.status.idle": "2024-02-23T13:49:53.447937Z",
     "shell.execute_reply": "2024-02-23T13:49:53.446964Z",
     "shell.execute_reply.started": "2024-02-23T13:49:37.433687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d330861cd7d41b5ad800f6da2de6409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d391d599b494601a5a7488d70a8dc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/127M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20e8378dded41269189d10bc634fb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9c5de921834b0989fb441cf9ef0779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/56167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae90b5c163d4499980977abf0c548b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c1611d-c14a-4138-8f12-fd3d342bcbf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:49:59.912515Z",
     "iopub.status.busy": "2024-02-23T13:49:59.912230Z",
     "iopub.status.idle": "2024-02-23T13:49:59.917526Z",
     "shell.execute_reply": "2024-02-23T13:49:59.916705Z",
     "shell.execute_reply.started": "2024-02-23T13:49:59.912491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 56167\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 6807\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72c3c32-feac-4187-bd65-39dc047708b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:50:02.933360Z",
     "iopub.status.busy": "2024-02-23T13:50:02.933080Z",
     "iopub.status.idle": "2024-02-23T13:50:03.303458Z",
     "shell.execute_reply": "2024-02-23T13:50:03.302905Z",
     "shell.execute_reply.started": "2024-02-23T13:50:02.933338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c18418dd5fe474782f4c8eb35708b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/56167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273c896c419d4b9580d936cc6dc5bccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fc022b7-5b85-4e21-a3cd-e51884435b7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-22T09:09:13.941868Z",
     "iopub.status.busy": "2024-02-22T09:09:13.941585Z",
     "iopub.status.idle": "2024-02-22T09:09:13.945439Z",
     "shell.execute_reply": "2024-02-22T09:09:13.944558Z",
     "shell.execute_reply.started": "2024-02-22T09:09:13.941846Z"
    }
   },
   "outputs": [],
   "source": [
    "# instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"cot_gsm8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850181b8-e4bc-485e-9552-baa9279035a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:50:07.887413Z",
     "iopub.status.busy": "2024-02-23T13:50:07.887122Z",
     "iopub.status.idle": "2024-02-23T13:50:07.892105Z",
     "shell.execute_reply": "2024-02-23T13:50:07.891346Z",
     "shell.execute_reply.started": "2024-02-23T13:50:07.887390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 34333\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'response', 'source'],\n",
       "        num_rows: 4771\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75367a14-ba59-4049-9473-ce72db3f5098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:50:11.552988Z",
     "iopub.status.busy": "2024-02-23T13:50:11.552666Z",
     "iopub.status.idle": "2024-02-23T13:50:11.558501Z",
     "shell.execute_reply": "2024-02-23T13:50:11.557728Z",
     "shell.execute_reply.started": "2024-02-23T13:50:11.552962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e2f48f-2c03-4c45-9dd1-199e4a34ab35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:50:14.734609Z",
     "iopub.status.busy": "2024-02-23T13:50:14.734322Z",
     "iopub.status.idle": "2024-02-23T13:50:14.739617Z",
     "shell.execute_reply": "2024-02-23T13:50:14.738915Z",
     "shell.execute_reply.started": "2024-02-23T13:50:14.734588Z"
    }
   },
   "outputs": [],
   "source": [
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a067c2b4-890f-4e88-bbc6-ce044f531d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:50:17.765602Z",
     "iopub.status.busy": "2024-02-23T13:50:17.765305Z",
     "iopub.status.idle": "2024-02-23T13:50:17.772178Z",
     "shell.execute_reply": "2024-02-23T13:50:17.771139Z",
     "shell.execute_reply.started": "2024-02-23T13:50:17.765577Z"
    }
   },
   "outputs": [],
   "source": [
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34d98e2-cea3-4a75-a1c4-4bac71cad75f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:50:26.420379Z",
     "iopub.status.busy": "2024-02-23T13:50:26.420102Z",
     "iopub.status.idle": "2024-02-23T13:50:26.425088Z",
     "shell.execute_reply": "2024-02-23T13:50:26.424298Z",
     "shell.execute_reply.started": "2024-02-23T13:50:26.420357Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    bos_token = \"<s>\"\n",
    "    original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "    response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "    input = sample[\"response\"]\n",
    "    eos_token = \"</s>\"\n",
    "\n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += \"### Instruction:\"\n",
    "    full_prompt += \"\\n\" + system_message\n",
    "    full_prompt += \"\\n\\n### Input:\"\n",
    "    full_prompt += \"\\n\" + input\n",
    "    full_prompt += \"\\n\\n### Response:\"\n",
    "    full_prompt += \"\\n\" + response\n",
    "    full_prompt += eos_token\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e39c485-eac6-4916-a99f-18573a147bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:50:46.034696Z",
     "iopub.status.busy": "2024-02-23T13:50:46.034379Z",
     "iopub.status.idle": "2024-02-23T13:50:46.041053Z",
     "shell.execute_reply": "2024-02-23T13:50:46.039958Z",
     "shell.execute_reply.started": "2024-02-23T13:50:46.034664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction\\nHow can I cook food while camping?\\n\\n### Response\\n',\n",
       " 'response': 'The best way to cook food is over a fire. You’ll need to build a fire and light it first, and then heat food in a pot on top of the fire.',\n",
       " 'source': 'dolly_hhrlhf'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruct_tune_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb620dbf-804d-4f45-9e59-5fe289969456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:51:03.625879Z",
     "iopub.status.busy": "2024-02-23T13:51:03.625575Z",
     "iopub.status.idle": "2024-02-23T13:51:03.631071Z",
     "shell.execute_reply": "2024-02-23T13:51:03.630389Z",
     "shell.execute_reply.started": "2024-02-23T13:51:03.625855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nThe best way to cook food is over a fire. You’ll need to build a fire and light it first, and then heat food in a pot on top of the fire.\\n\\n### Response:\\nHow can I cook food while camping?</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(instruct_tune_dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b52fbc5-9f90-43bb-baa3-21623508e2d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:51:11.924732Z",
     "iopub.status.busy": "2024-02-23T13:51:11.924426Z",
     "iopub.status.idle": "2024-02-23T13:54:09.386186Z",
     "shell.execute_reply": "2024-02-23T13:54:09.385227Z",
     "shell.execute_reply.started": "2024-02-23T13:51:11.924707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a7672dffae40369ba009e40bdd9a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3549b3bae4418389e5717135b78cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5be7ef56af487295b9bdb96aecc73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f9c195d750460f85a88a760dc0d218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7265f4e5def049a2aee92452d0f68620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eb875fdbb442dd9e500d1b123b327b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed83c2c646e848d0b5796423fe999794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc5f6961f544f5fb0263d2a046e4a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b6dc6193064eb1bbd4cdd6d76a7a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dff92bbdfed400fa6e2a4fecc2ebaea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b75a57a1c34d9b9cf109229fc95fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9866cbe1-a4e1-47a7-af29-ffc33b36ea92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:54:19.497107Z",
     "iopub.status.busy": "2024-02-23T13:54:19.496293Z",
     "iopub.status.idle": "2024-02-23T13:54:19.502581Z",
     "shell.execute_reply": "2024-02-23T13:54:19.501260Z",
     "shell.execute_reply.started": "2024-02-23T13:54:19.497072Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "    encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f742e8-57ab-4107-b51c-1cbfbf5f24d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:54:24.728069Z",
     "iopub.status.busy": "2024-02-23T13:54:24.727432Z",
     "iopub.status.idle": "2024-02-23T13:54:35.246354Z",
     "shell.execute_reply": "2024-02-23T13:54:35.245612Z",
     "shell.execute_reply.started": "2024-02-23T13:54:24.728037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> \\n\"Grasses are an incredibly diverse group of plants, with over 12,000 known species. The most popular type grown due to its rapid growth, ease of cultivation, and soft texture is Kentucky Bluegrass. On the other hand, Ryegrass has a bright green, shiny appearance. Fescues are a dark green shade and also have a shiny surface. Bermuda grass is harder and can grow well in drier soils. These are just a few examples of the different types of grasses that exist.\"</s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\\n\\n### Response:\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b17828-1c71-4bc8-b806-a535cca65b13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:54:40.533681Z",
     "iopub.status.busy": "2024-02-23T13:54:40.533208Z",
     "iopub.status.idle": "2024-02-23T13:54:42.575639Z",
     "shell.execute_reply": "2024-02-23T13:54:42.575084Z",
     "shell.execute_reply.started": "2024-02-23T13:54:40.533657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> \\nTo cook food over a fire, you first need to build a fire and light it. After that, you can heat your food in a pot placed on top of the fire.</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThe best way to cook food is over a fire. You’ll need to build a fire and light it first, and then heat food in a pot on top of the fire.\\n\\n### Response:\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb8d53ea-6460-40fd-9007-15dd512b414f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:54:55.170704Z",
     "iopub.status.busy": "2024-02-23T13:54:55.169857Z",
     "iopub.status.idle": "2024-02-23T13:54:55.204296Z",
     "shell.execute_reply": "2024-02-23T13:54:55.203712Z",
     "shell.execute_reply.started": "2024-02-23T13:54:55.170675Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f2f2b83-a2c4-4199-b0e4-17982d2dcb76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:55:01.004655Z",
     "iopub.status.busy": "2024-02-23T13:55:01.003864Z",
     "iopub.status.idle": "2024-02-23T13:55:01.705146Z",
     "shell.execute_reply": "2024-02-23T13:55:01.704364Z",
     "shell.execute_reply.started": "2024-02-23T13:55:01.004627Z"
    }
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b03bc4a-234b-4979-9732-1791c37844af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:55:07.624234Z",
     "iopub.status.busy": "2024-02-23T13:55:07.623644Z",
     "iopub.status.idle": "2024-02-23T13:55:07.642597Z",
     "shell.execute_reply": "2024-02-23T13:55:07.641859Z",
     "shell.execute_reply.started": "2024-02-23T13:55:07.624210Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"mistral_instruct_generation\",\n",
    "  #num_train_epochs=5,\n",
    "  max_steps = 100, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 4,\n",
    "  warmup_steps = 0.03,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=2e-4,\n",
    "  bf16=True,\n",
    "  lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdfea4f6-df6f-48a5-ad9e-c861982f3be4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:55:15.285607Z",
     "iopub.status.busy": "2024-02-23T13:55:15.284773Z",
     "iopub.status.idle": "2024-02-23T13:55:17.946676Z",
     "shell.execute_reply": "2024-02-23T13:55:17.945871Z",
     "shell.execute_reply.started": "2024-02-23T13:55:15.285581Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ddb192b17c47c3b17dda1b9b3a9e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ac21919330485b8fb66858edf41bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt,\n",
    "  args=args,\n",
    "  train_dataset=instruct_tune_dataset[\"train\"],\n",
    "  eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6497d8f5-4eb7-43d3-818a-1b3014eb01fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T13:55:28.685691Z",
     "iopub.status.busy": "2024-02-23T13:55:28.684917Z",
     "iopub.status.idle": "2024-02-23T14:12:26.602123Z",
     "shell.execute_reply": "2024-02-23T14:12:26.601440Z",
     "shell.execute_reply.started": "2024-02-23T13:55:28.685655Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3df9a40c11440e854e8eadfe34387d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666923357794682, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240223_135722-3lgfqvdg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/smukherjee1/huggingface/runs/3lgfqvdg\" target=\"_blank\">glowing-mandu-6</a></strong> to <a href=\"https://wandb.ai/smukherjee1/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 13:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.510600</td>\n",
       "      <td>1.373599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.424000</td>\n",
       "      <td>1.333582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.444900</td>\n",
       "      <td>1.322162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.452800</td>\n",
       "      <td>1.312718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.416900</td>\n",
       "      <td>1.306690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory mistral_instruct_generation/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016.097580909729\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "print(time.time()- start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83c8fe65-f619-4650-b8ab-2ff0265fd6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T14:14:07.051679Z",
     "iopub.status.busy": "2024-02-23T14:14:07.051360Z",
     "iopub.status.idle": "2024-02-23T14:14:26.380962Z",
     "shell.execute_reply": "2024-02-23T14:14:26.380214Z",
     "shell.execute_reply.started": "2024-02-23T14:14:07.051653Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"mistral_instruct_generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba2258de-3ef9-48e4-ba2b-bd295fa4eeaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T14:14:34.386989Z",
     "iopub.status.busy": "2024-02-23T14:14:34.386533Z",
     "iopub.status.idle": "2024-02-23T14:14:36.964944Z",
     "shell.execute_reply": "2024-02-23T14:14:36.964285Z",
     "shell.execute_reply.started": "2024-02-23T14:14:34.386967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/peft/tuners/lora/bnb.py:249: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1996eee-7702-4c0f-9333-74df88c166c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T14:14:39.437789Z",
     "iopub.status.busy": "2024-02-23T14:14:39.437407Z",
     "iopub.status.idle": "2024-02-23T14:14:39.444126Z",
     "shell.execute_reply": "2024-02-23T14:14:39.443231Z",
     "shell.execute_reply.started": "2024-02-23T14:14:39.437766Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "    encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    return decoded_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "995294df-f435-41c1-9953-fc9b30ca5520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T14:18:12.824075Z",
     "iopub.status.busy": "2024-02-23T14:18:12.823053Z",
     "iopub.status.idle": "2024-02-23T14:18:13.754042Z",
     "shell.execute_reply": "2024-02-23T14:18:13.753304Z",
     "shell.execute_reply.started": "2024-02-23T14:18:12.824034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> ### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\\n\\n### Response:\\nWhich type of grass is the most common and why is it popular?</s>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\\n\\n### Response:\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bb3440a-de5c-4113-b792-b4ff7efb88a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T14:24:53.275792Z",
     "iopub.status.busy": "2024-02-23T14:24:53.275020Z",
     "iopub.status.idle": "2024-02-23T14:24:56.205067Z",
     "shell.execute_reply": "2024-02-23T14:24:56.204371Z",
     "shell.execute_reply.started": "2024-02-23T14:24:53.275750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> ### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThe best way to look for something is to find it within you.\\n\\n### Response:\\nTo find something is generally best done through introspection and exploring one's own thoughts and feelings. This can help you understand if the thing you are searching for is something you truly need or if it's just a fleeting desire that will pass.</s>\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThe best way to look for something is to find it within you.\\n\\n### Response:\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97af0f81-efcf-4eb3-b830-372594d322b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T14:19:05.280920Z",
     "iopub.status.busy": "2024-02-23T14:19:05.280256Z",
     "iopub.status.idle": "2024-02-23T14:19:05.287928Z",
     "shell.execute_reply": "2024-02-23T14:19:05.287111Z",
     "shell.execute_reply.started": "2024-02-23T14:19:05.280894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bd0f5-cb6b-4273-9945-6a78358fab5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
